\documentclass[t]{beamer}

% Load general definitions
\input{../defs/preamble.tex}

% Specific definitions
\title[]{Design and Analysis of Experiments}
\subtitle[]{07 - Paired Design}
\author[]{Felipe Campelo\\{\footnotesize http://orcslab.cpdee.ufmg.br/}}
\institute{Graduate Program in Electrical Engineering}
\date{\scriptsize Belo Horizonte\\May 2018}

\begin{document}

% cover page
\setbeamertemplate{footline}{}
\begin{frame}
\begin{flushright}
\includegraphics[width=.25\textwidth]{../figs/principal_completa3_ufmg}
\end{flushright}
  \titlepage
  \begin{tikzpicture}[remember picture,overlay]
  \node[anchor=south east,xshift=-5pt,yshift=122pt] at (current page.south east) {\tiny Version 2.12.2018};
  \node[anchor=south west,yshift=0pt] at (current page.south west) {\includegraphics[width=.15\textwidth]{../figs/by-nc-sa.png}};
  \end{tikzpicture}  
\end{frame}

%=====

% quotation page
  \begin{frame}[b]
		\frametitle{}
\begin{columns}[T]
\column{0.7\textwidth}
\flushright{\small ``\textit{Don't be afraid of hard work.\\
Nothing worthwhile comes easily.\\
Don't let others discourage you or tell you\\
that you can't do it. In my day I was told\\
women didn't go into chemistry. I saw\\
no reason why we couldn't.}''\\\ \\
Gertrude B. Elion\\1918 - 1999\\American biochemist and pharmacologist.\\}
\column{0.3\textwidth}
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south east,yshift=32pt,xshift=0pt] at (current page.south east)
{\includegraphics[width=\textwidth]{../figs/Gertrude.png}};
\end{tikzpicture}
\end{columns}
\vhalf
\lfr{Image by Rachel Ignotofsky: \textit{Women in Science}, 2016} \lfr{\url{https://www.rachelignotofskydesign.com/women-in-science/}}
\end{frame}

%=====
% Main slides

\begin{ftst}
{Comparison of two means}
{Dependent populations}
Suppose the following situation: a young researcher develops an optimization algorithm (A) for a given family of problems, and wants to compare its convergence speed against a method that represents the state-of-the-art (B).
\vone
The researcher implements both methods and wants to determine whether the proposed one has a better average performance for problems of that particular family, represented by a given benchmark set.
\vone
The measurements are made under homogeneous conditions (same computer, same operational conditions, etc.) and the time is measured in a way that is not sensitive to other processes running in the system.
\vone
%\lfr{Image: \url{http://goo.gl/xwqig9}}
%\begin{tikzpicture}[remember picture,overlay]
%\node[anchor=south east,yshift=-5pt,xshift=5pt] at (current page.south east)
%{\includegraphics[width=.3\textwidth]{../figs/algoresearcher.png}};
%\end{tikzpicture}
\end{ftst}

%=====

\begin{ftst}
{Comparison of two means}
{Dependent populations}
This problem has some important questions worth considering:

\bitems What is the actual question of interest?
\spitem What is the \textit{population} for which that question is relevant?
\spitem What are the independent observations for that population?
\spitem What are the relevant sample sizes for the experiment?
\eitem
\end{ftst}

%=====

\begin{ftst}
{Comparison of two means}
{Paired design}
The variability due to the different test problems is a strong source of spurious variation that can and must be controlled;
\vone
An elegant solution to eliminate the influence of this nuisance parameter is the \textit{pairing} of the measurements by problem:

\bitems Observations are considered in pairs (A, B) for each problem; 
\item Hypothesis testing is done on the sample of \textit{differences};
\eitem
\end{ftst}

%=====

\begin{ftst}
{Comparison of two means}
{Paired design}
Let $y_{Aj}$ and $y_{Bj}$ denote paired observations of average time for methods A and B, for each problem instance $j$. The \textit{paired differences} of the observations are simply $d_j = y_{Aj} - y_{Bj}$.
\vone
If we model our observations as an additive process:
\beqs
y_{ij} = \underbrace{\mu + \tau_i}_{\mu_i} + \beta_j + \varepsilon_{ij}
\eqs
\noindent where $\mu$ is the grand mean, $\tau_i$ is the effect of the $i$-th algorithm on the mean, $\beta_j$ is the effect of the $j$-th problem, and $\varepsilon_{ij}$ is the model residual, then:
\beqs
\begin{split}
d_j &= \cancelto{0}{\left(\mu +\beta_j - \mu - \beta_j\right)} + \tau_A-\tau_B + \varepsilon_{Aj}-\varepsilon_{Bj}\\
&=\mu_{D} + \varepsilon_j\\
\end{split}
\eqs
\end{ftst}

%=====

\begin{ftst}
{Comparison of two means}
{Paired design}
The hypotheses of interest can now be defined in terms of $\mu_D$, e.g.:
\beqs\begin{cases}
H_0: \mu_D = 0\\
H_1: \mu_D \neq 0
\end{cases}\eqs
\noindent which can now be treated as a test of hypotheses for a single sample: the population of interest is the differences in average times until convergence for the problems under investigation. The test statistic is given by:
\beqs T_0 = \frac{\bar{D}}{S_D/\sqrt{N}}\eqs 
\vhalf
which is distributed under the null hypothesis as a Student-t variable with $N-1$ degrees of freedom (where $N$ is the number of test problem instances in the experiment);
\end{ftst}

%=====

\begin{ftst}
{Comparison of two means}
{Paired design}
Some other important questions worth considering:

\bitems In this example the minimally interesting effect size $\delta^*$ must be expressed in terms of \textit{average time gains across problems} (not within individual instances);
\spitem The most important sample size to consider in this situation refers to the \textit{number of problem instances}, and not necessarily to the number of within-problems repeated measures;
\spitem The number of repetitions within each problem will have an impact on the uncertainty associated to each observation (that is, to each value of mean time to convergence for each algorithm on each problem), which will propagate down to the residual variance.
\eitem
\end{ftst}

%=====

\begin{ftst}
{Comparison of two means}
{Paired design}
Some other important questions worth considering:

\bitems Pairing removes the effects of controllable nuisance factors from the analysis.
\spitem
Strongly indicated in cases with strong correlations between samples (e.g., heterogeneous experimental conditions).
\eitem
\end{ftst}

%=====

\begin{ftst}
{Comparison of two means}
{Paired design}
Going back to our example, assume the following facts about the desired comparison:

\bitems The benchmark set is composed of seven problems ($N=7$);
\spitem The researcher is interested in finding differences in mean time to convergence greater than ten seconds ($\delta^*=10$) with a power of at least $(1-\beta)=0.8$, using a significance level $\alpha=0.05$;
\spitem The researcher performs $n=30$ repeated runs\footnote[1]{\tiny Not that this number is necessarily good, but it is generally an easy alternative if you don't want to keep justifying your choices to less statistically-savvy reviewers.} of each algorithm in each problem, from random initial conditions.
\eitem

%\begin{tikzpicture}[remember picture,overlay]
%\node[anchor=south east,yshift=-5pt,xshift=5pt] at (current page.south east)
%{\includegraphics[width=.3\textwidth]{../figs/algoresearcher.png}};
%\end{tikzpicture}
\end{ftst}

\begin{ftstf}
{Comparison of two means}
{Paired design}
Step 1: load and precondition the data.
\begin{rcode}
> # Read data
> data <- read.table("../data files/soltimes.csv",
+                    header=T)

# "Problem" is a categorical variable, not a contiunous one
> data$Problem <- as.factor(data$Problem)

# Summarize within-problem observations by mean
> aggdata <- aggregate(Time ~ Problem:Algorithm,
+                      data = data,
+                      FUN  = mean)
> summary(aggdata)
 Problem Algorithm      Time       
 1:2     A:7       Min.   : 37.63  
 2:2     B:7       1st Qu.:109.45  
 3:2               Median :178.73  
 4:2               Mean   :175.48  
 5:2               3rd Qu.:245.25  
 6:2               Max.   :296.79  
 7:2                               

\end{rcode}
\end{ftstf}

%=====

\begin{ftstf}
{Comparison of two means}
{Paired design}
Step 2: analysis
\begin{rcode}
> # Perform paired t-test
> t.test(Time ~ Algorithm,
+        paired = TRUE,
+        data   = aggdata)

Paired t-test
data:  Time by Algorithm
t = -9.1585, df = 6, p-value = 9.54e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -21.85862 -12.64118
sample estimates:
mean of the differences 
               -17.2499 
\end{rcode}
\end{ftstf}

%=====

\begin{ftstf}
{Comparison of two means}
{Paired design}
Alternatively, we could have done:
\begin{rcode}
> difTimes <- aggdata$Time[1:7] - aggdata$Time[8:14])
> t.test(difTimes)


One Sample t-test
data:  difTimes
t = -9.1585, df = 6, p-value = 9.54e-05
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 -21.85862 -12.64118
sample estimates:
mean of x 
 -17.2499 
\end{rcode}
\end{ftstf}

%=====

\begin{ftstf}
{Comparison of two means}
{Paired design}
Verify assumptions:
\begin{rcode}
> shapiro.test(difTimes)

Shapiro-Wilk normality test
data:  difTimes
W = 0.8387, p-value = 0.09655

# Redo test without outlier
> indx <- which(difTimes == max(difTimes))
> t.test(difTimes[-indx])$p.value
[1] 6.179743e-06
> t.test(difTimes[-indx])$conf.int
[1] -21.41856 -16.48037
\end{rcode}
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south east,yshift=-10pt,xshift=10pt] at (current page.south east) {\includegraphics[width=.5\textwidth]{../figs/soltimesqq.pdf}};
\end{tikzpicture}
\end{ftstf}

%=====

\begin{ftstf}
{Comparison of two means}
{Paired design}
What happens if we fail to consider the problem effects?
\begin{rcode}
> t.test(Time ~ Algorithm, data = aggdata)

Welch Two Sample t-test
data:  Time by Algorithm
t = -0.3609, df = 11.993, p-value = 0.7245
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -121.40320   86.90341
sample estimates:
mean in group A mean in group B 
       166.8527        184.1026 
\end{rcode}
\end{ftstf}

%=====

\begin{ftstf}
{Comparison of two means}
{Paired design}
Paired designs can require smaller sample sizes for equivalent power in cases where the between-units (in our example, the between-problems) variation is relatively high;
\vhalf
More specifically, if the within-level variation is given by $\sigma_\epsilon$ and the between-units variation is $\sigma_u$, we have that, for large enough $N$\\(e.g., $N\geq 10$),
\beqs
\frac{N_{\mbox{\tiny unpaired}}}{N_{\mbox{\tiny paired}}}\approx\sqrt{2\left[\left(\frac{\sigma_u}{\sigma_\epsilon}\right)^2+1\right]}
\eqs
\vhalf
Failure to consider inter-unit variability can result in the masking of relevant effects by the nuisance factor.
\vhalf
Similarly, failure in recognizing the dependence structure of within-unit measurements yields tests with artificially inflated degrees of freedom, which results in the inflation of the effective value of $\alpha$.
\end{ftstf}

%=====

\begin{ftst}
{Bibliography}
{\ }
\scriptsize
\textbf{Required reading}

\benums D.C. Montgomery, G.C. Runger, \textit{Applied Statistics and Probability for Engineers}, Ch. 10. 5th ed., Wiley, 2010. 
\item M.J. Crawley, \textit{The R Book}, Ch. 8. 1st ed., Wiley, 2007;
\eenum

\textbf{Recommended reading}

\benums L. Lehe and V. Powell, \textit{Simpson's Paradox} - \url{http://vudlab.com/simpsons/} 
\item J.P. Simmons, L.D. Nelson, and U. Simonsohn, \textit{False-Positive Psychology : Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant}, Psychological Science 22(11):1359-1366, 2011 - \url{http://goo.gl/9e0cdw}
\eenum
\end{ftst}

%=====

\input{../defs/FinalSlide.tex}


\end{document}