\documentclass[t]{beamer}

% Load general definitions
\input{../defs/preamble.tex}

% Specific definitions
\title[]{Design and Analysis of Experiments}
\subtitle[]{10 - Analysis of Variance}
\author[]{Felipe Campelo\\{\footnotesize http://orcslab.cpdee.ufmg.br/}}
\institute{Graduate Program in Electrical Engineering}
\date{\scriptsize Belo Horizonte\\October 2018}

\begin{document}

% cover page
\setbeamertemplate{footline}{}
\begin{frame}
\begin{flushright}
\includegraphics[width=.25\textwidth]{../figs/principal_completa3_ufmg}
\end{flushright}
  \titlepage
  \begin{tikzpicture}[remember picture,overlay]
  \node[anchor=south east,xshift=-5pt,yshift=122pt] at (current page.south east) {\tiny Version 2.12.2018b};
  \node[anchor=south west,yshift=0pt] at (current page.south west) {\includegraphics[width=.15\textwidth]{../figs/by-nc-sa.png}};
  \end{tikzpicture}  
\end{frame}

%=====

% quotation page
  \begin{frame}[b]
		\frametitle{}
\begin{columns}[T]
\column{0.65\textwidth}
\flushright{\small ``\textit{The attempt to correlate all the known\\
														phenomena, and to bind them together into\\
														one consistent whole, led to the deduction of\\
														new facts, which, when duly tested by\\
														experiment, became parts of the growing body,\\
                                                       and, themselves, opened up fresh questions,\\
                                                       to be answered in their turn by experiment.}''\\\ \\\ \\
Hertha Ayrton\\
1854--1923\\
British engineer, mathematician and physicist\\\ \\}
\column{0.35\textwidth}
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south east,yshift=26pt,xshift=0pt] at (current page.south east)
{\includegraphics[width=\textwidth]{../figs/hertha.jpg}};
\end{tikzpicture}
\end{columns}
\vhalf
\lfr{Image by George Doutsiopoulos:} 
\lfr{\url{https://www.behance.net/gallery/56913255/STEM-Epic-Heroes-Engineering-Heroes-(part-1)}}
\end{frame}

%=====
% Main slides

%% Things to add in the future:
% Sample size calculations for ANOVA (first) with additional observations for post-hoc tests (if needed)
% Heteroscedasticity correction - Welch ANOVA 

\begin{ftst}
{Comparison of multiple means}
{Introduction}
In the previous sections, we have (hopefully) developed a solid understanding of the main concepts associated with comparing the means of two groups;
\vone
There are many cases, however, in which one may want to perform inference about differences of the means of multiple populations;
\vone
We will develop the main concepts and ideas related with this kind of test by examining a simple example, related to a paper manufacturing operation.
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south east,yshift=10pt,xshift=-10pt] at (current page.south east)
{\includegraphics[width=.3\textwidth]{../figs/paper.jpg}};
\end{tikzpicture}
\lfr{Image: \url{http://goo.gl/xYVW0M}}
\end{ftst}

%=====

\begin{ftst}
{Example: paper manufacturing}
{Problem definition}
Tensile strength (TS) is an important characteristic for certain types of paper for industrial use;
\vone
A reasonable conjecture is that this characteristic is influenced by the kind of wood fiber used in the manufacturing process.
\vone
The process engineer wants to investigate whether four different wood fibers result in papers with relevant differences of TS, using a pilot plant as his experimental unit.
\lfr{Example adapted from Montgomery \& Runger (2010), Ch. 13.}
\end{ftst}

%=====

\begin{ftst}
{Example: paper manufacturing}
{Problem definition}
Suppose that the total budget allocated for the experiment allows only six production runs for each kind of wood fiber.
\vone
Under these specifications, the experiment has a single experimental \textit{factor} (\textit{wood fiber}) with $a = 4$ \textit{levels} (fiber types \textit{A}, \textit{B}, \textit{C} and \textit{D}) and $n = 6$ \textit{replicates} at each level.
\vone
The response variable will be the tensile strength of paper (measured, e.g., in kPa). The engineering team is interested in finding out whether any fiber type leads to an increase in the mean TS value of the paper. 
\vone
The minimum difference of practical meaning is defined as $5 kPa$, and a reasonable upper estimate for the standard deviation of this process is $\hat{\sigma} = 6 kPa$. Desired error levels are defined as $\alpha = 0.1$ and $\beta = 0.2$.
\end{ftst}

%=====

\begin{ftstf}
{Example: paper manufacturing}
{Exploratory data analysis}
\begin{rcode}
> paper <- read.table(file   = "../data files/paper_strength.csv", 
+                     header = TRUE, 
+                     sep    = ",")
 
> library(ggplot2)
> ggplot(paper, 
+     aes(x    = Fiber.type,
+         y    = TS.kPa,
+         fill = Fiber.type)) + 
+  geom_boxplot() + 
+  geom_point()

\end{rcode}
%\vhalf
%Graphical analysis suggests the existence of an effect of the factor on the response variable;
%\vhalf
%Besides, a relative symmetry and a fairly consistent variance can be observed in all samples.
\begin{tikzpicture}[remember picture,overlay]
\node[anchor= east,yshift=-40pt,xshift=0pt] at (current page.east) {\includegraphics[width=.55\textwidth]{../figs/paperbox.png}};
\end{tikzpicture}
\end{ftstf}

%=====

\begin{ftst}
{Example: paper manufacturing}
{Exploratory data analysis}
The boxplot suggests the existence of differences among factor levels;
\vone
Besides, we can also observe a small variability in the spread of different levels; some suggestion of asymmetry in level \textit{B}; and a possible outlier in level \textit{C}.
\vone
These characteristics will need to be\\
taken into account during the analysis.
\begin{tikzpicture}[remember picture,overlay]
\node[anchor= south east,yshift=0pt,xshift=0pt] at (current page.south east) {\includegraphics[width=.45\textwidth]{../figs/paperbox.png}};
\end{tikzpicture}
\end{ftst}

%=====

\begin{ftst}
{Example: paper manufacturing}
{Statistical model}
This data can be described by a linear statistical model of the form:
\beqs
y_{ij} = \underbrace{\mu_i + \epsilon_{ij}}_{\mbox{\scriptsize Means model}} = \underbrace{\mu + \tau_i + \epsilon_{ij}}_{\mbox{\scriptsize Effects model}}\begin{cases}i=1,\ldots,a\\j=1,\ldots,n\end{cases}
\eqs
\vhalf
\noindent where $\mu$ is the overall mean, $\tau_i$ represents the effect of the $i$-th level, and $\epsilon_{ij}$ is the residual (random error, or unmodeled variability);
\vhalf
In the derivation of the statistical test for the existence of differences in the group means, we will employ the effects model, and initially consider a few assumptions about the residuals:
\beqs
y_{ij} = \mu + \tau_i + \epsilon_{ij}\begin{cases}i=1,\ldots,a\\j=1,\ldots,n\end{cases},\ \ \ \mbox{with }\epsilon_{ij} \overset{\mbox{\tiny i.i.d.}}{\sim}\mathcal{N}\left(0,\sigma^2\right)
\eqs
\end{ftst}

%=====

\begin{ftst}
{Example: tensile strength}
{Statistical model}
If these assumptions are correct, the populations are expected to be distributed as:

\centering\includegraphics[width=0.7\textwidth]{../figs/models.png}

\vspace{-1em}
\flushleft Since we are interested in testing our data for differences in the mean values of each population, the test hypotheses can be described as:
\beqs
\begin{cases}
H_0: \tau_i = 0,\ \ \forall i\in\left\{1,2,\ldots,a\right\}\\
H_1: \exists\ \tau_i \neq 0
\end{cases}\ \ \ \ \ \ \ 
\eqs

If data collection is performed in random order, under constant experimental conditions, we have a \textit{completely randomized design}.
\lfr{Image: Montgomery\&Runger (2010), Ch. 13}
\end{ftst}

%=====

\begin{ftst}
{The Fixed Effects Model}
{Definition}
This approach to modeling the mean effects of specific factor levels is known as the \textit{fixed effects model};
\vone
This approach is appropriate to testing hypotheses in situations when factor levels are arbitrarily defined by the experimenter;
\vone
For these cases, the inference is made over the mean values for each level, and cannot be extended to similar levels that were not tested (e.g., other types of wood fiber);
\vone
Other situations may require different kinds of modeling, such as \textit{random} or \textit{mixed effects models}, but these will not be explored here.
\end{ftst}

%=====

\begin{ftst}
{The Fixed Effects Model}
{Development}
As mentioned earlier, we will use the \textit{effects model} for describing the development of the statistical test: 
\beqs y_{ij} = \mu + \tau_i + \epsilon_{ij}\begin{cases}i=1,\ldots,a\\j=1,\ldots,n\end{cases}\eqs
\vhalf
\noindent where treatment effects are seen as deviations from the grand mean $\mu$.
\vone
By construction, we have that:
\beqs
\sum_{i=1}^{a}\tau_i = 0;
\eqs 
\end{ftst}

%=====

\begin{ftst}
{The Fixed Effects Model}
{Development}
The total variability of the data can be expressed by the \textit{total sum of squares}, which represents the sum of the squared deviations between each observation and the overall sample mean:
\beqs
SS_T = \sum_{i=1}^{a}{\sum_{j=1}^{n}{\left(y_{ij} - \bar{y}_{\bullet\bullet}\right)^2}}
\eqs
With some relatively simple algebra, the $SS_T$ can be divided into two terms, representing the within-group and the between-group variability:
\beqs
SS_T = \sum_{i=1}^{a}{\sum_{j=1}^{n}{\left(y_{ij} - \bar{y}_{\bullet\bullet}\right)^2}}=\underbrace{n\sum_{i=1}^{a}{\left(\bar{y}_{i\bullet} - \bar{y}_{\bullet\bullet}\right)^2}}_{SS_{\mbox{\tiny\textit{Levels}}}} + \underbrace{\sum_{i=1}^{a}{\sum_{j=1}^{n}{\left(y_{ij}- \bar{y}_{i\bullet}\right)^2}}}_{SS_E}
\eqs
\vhalf
\noindent where $\bullet$ indicates the summation over an index, and $\bar{\ \ }$ indicates an averaging operation.
\end{ftst}

%=====

\begin{ftst}
{The Fixed Effects Model}
{Development}
Dividing the sums of squares by their respective number of degrees of freedom yields quantities known as \textit{mean squares}.
\vhalf
The relevant means squares for our test will be the \textit{levels mean square} and the \textit{residual mean square}:
\beqs
MS_E = \frac{SS_E}{a\left(n-1\right)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
MS_{\mbox{\tiny\textit{Levels}}} = \frac{SS_{\mbox{\tiny\textit{Levels}}}}{a-1}
\eqs
\vhalf
The expected values of these quantities are:
\beqs
E\left[MS_E\right] = \sigma^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ E\left[MS_{\mbox{\tiny\textit{Levels}}}\right] = \sigma^2 + \frac{n\sum_{i=1}^{a}{\tau_i^2}}{a-1}
\eqs

\end{ftst}

%=====

\begin{ftst}
{The Fixed Effects Model}
{Development}
\beqs
E\left[MS_E\right] = \sigma^2\ \ \ \ \ \ \ \ \ \ \ \ \ \ E\left[MS_{\mbox{\tiny\textit{Levels}}}\right] = \sigma^2 + \frac{n\sum_{i=1}^{a}{\tau_i^2}}{a-1}
\eqs
\vone
Notice that $MS_E$ is an unbiased estimator for the common variance of the residuals, while $MS_{\mbox{\tiny\textit{Levels}}}$ is biased by a term that is proportional to the squared values of the $\tau_i$ coefficients.
\vone
However, under $H_0$ we have that $\tau_i=0$ for all $i$, that is, $E\left[MS_{\mbox{\tiny\textit{Levels}}}\right] = E\left[MS_{E}\right] = \sigma^2$. \alert{\textit{But only if the null hypothesis is true}}.
\end{ftst}

%=====

\begin{ftst}{The Fixed Effects Model}{Development}
It can be shown that, if $H_0$ is true, the statistic
\beqs
F_0 = \frac{MS_{\mbox{\tiny\textit{Levels}}}}{MS_E} 
\eqs
\vhalf
\noindent is distributed according to an $F$ distribution with $a-1$ degrees of freedom for the numerator and $a(n-1)$ for the denominator.  The usual notation is $F_{\left(a-1\right),a(n-1)}$
\vone
If $H_0$ is false, the expected value of $MS_{\mbox{\tiny\textit{Levels}}}$ is larger than that of $MS_E$, which results in larger values of $F_0$ and defines the critical region for our test:
\vhalf
\begin{block}{}
\centering Reject $H_0$ at the $\alpha$ significance level if\\$f_0>F_{1-\alpha;(a-1),a(n-1)}$
\end{block}
\end{ftst}

%=====

\begin{ftstf}
{Example: paper manufacturing}
{Computational analysis}
\begin{rcode}
> my.model <- aov(TS.kPa ~ Fiber.type, 
+              data = paper)
>
> summary.aov(my.model)

Df Sum Sq Mean Sq F value   Pr(>F)    
Fiber.type   3 110.77   36.92   13.62 4.56e-05 ***
Residuals   20  54.24    2.71                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{rcode}
\vhalf
The \textit{ANOVA table} provides information on the sources of variation, together with their corresponding \textit{d.o.f.}, sums of squares and mean square values. The table also informs the values of the test statistic and the corresponding p-value of the test ($Pr(>F)$).
\vone
In this case, the p-value ($p = 4.56\times 10^{-5}$) suggests the rejection of the null hypothesis in favor of the alternative. But what does that mean?
\end{ftstf}

%=====

\begin{ftst}
{Example: paper manufacturing}
{Computational analysis}
Recall the null and alternative hypotheses for the ANOVA:
\beqs
\begin{cases}
H_0: \tau_i = 0,\ \ \forall i\\
H_1: \exists\ \tau_i \neq 0
\end{cases}
\eqs
\vhalf
The rejection of the null hypothesis leads to the conclusion that \textit{there is at least one level with an effect significantly different from zero}. But which one?
\vhalf
For this analysis to be complete, we still need to answer two questions:
\vhalf
\bitems Can we verify the assumptions of the test?
\spitem Which means are different from which, and by how much?
\eitem
\end{ftst}

%=====

\begin{ftstf}
{Assumptions}
{Model validation}
As mentioned earlier, the ANOVA model is based on three assumptions on the behavior of the residuals:
\bitems \textit{Independence};
\item \textit{Homoscedasticity}, i.e., equality of variances across groups;
\item \textit{Normality};
\eitem
\vhalf
The residuals of the model can be easily obtained as:
\beqs
e_{ij} = y_{ij} - \hat{y}_{ij} = y_{ij} - \left(\hat{\mu} + \hat{\tau_i}\right) = y_{ij}-\bar{y}_{i\bullet}
\eqs
\vhalf
\noindent or extracted directly from the fitted object in R using \verb|my.model$residuals|.

\end{ftstf}

%=====

\begin{ftstf}
{Assumptions}
{Model validation}
The normality assumption can be tested using the Shapiro-Wilk test coupled with a normal QQ plot of the residuals.
\begin{rcode}
> shapiro.test(model$residuals)
Shapiro-Wilk normality test
data:  my.model$residuals
W = 0.9722, p-value = 0.7225

> library(car)
> qqPlot(my.model$residuals, 
pch = 16, 
lwd = 3, 
cex = 2, 
las = 1)
\end{rcode}
\vskip 3em
The ANOVA is relatively robust to moderate violations of normality, as long as the other assumptions are verified (or the sample size is large enough).
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south east,yshift=50pt,xshift=0pt] at (current page.south east) {\includegraphics[width=.45\textwidth]{../figs/paperqq.png}};
\end{tikzpicture}
\end{ftstf}

%=====

\begin{ftstf}
{Assumptions}
{Model validation}
The homoscedasticity assumption can be verified by the Fligner-Killeen test, together with plots of residuals by fitted values:
\begin{rcode}
> fligner.test(TS_kPa ~ Fiber.type, data = paper)
Fligner-Killeen test of homogeneity of
variances
data:  data:  TS.kPa by Fiber.type
Fligner-Killeen:
med chi-squared = 1.0622, df = 3, 
p-value = 0.7862
>
> plot(x = my.model$fitted.values,
+      y = my.model$residuals) 

\end{rcode}
\vhalf
ANOVA is relatively robust to modest\\
violations of homoscedasticity, as far\\
as the sample is \textit{balanced}.
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north east,yshift=-80pt,xshift=0pt] at (current page.north east) {\includegraphics[width=.5\textwidth]{../figs/papervar.png}};
\end{tikzpicture}
\end{ftstf}

%=====

\begin{ftstf}
{Assumptions}
{Model validation}
As usual, the independence assumption should be guaranteed (to the best of the experimenter's knowledge) on the design phase, as well as on the analysis. This includes avoiding pseudoreplication and ordering effects, among others.
\vone
To test for serial correlations, we can use the Durbin-Watson test, but that only really makes sense if the data is presented to the DW test ordered by an unmodelled and possibly influential variable (such as by order of data collection).
%\vhalf
%\begin{rcode}
%> durbinWatsonTest(model)
% lag Autocorrelation D-W Statistic p-value
%   1    -0.008996302      1.872801   0.868
% Alternative hypothesis: rho != 0
%
%> plot(x    = seq_along(model$residuals),
%+      y    = model$residuals,
%+      type = "l", ...)
%> points(x    = seq_along(model$residuals),
%+        y    = model$residuals,
%+        type = "p",
%+        col  = as.numeric(paper[,1]), ...)
%\end{rcode}
\vone
The ANOVA can be quite sensitive to violations of independence. Randomization and attention to possibly influential factors can help avoiding violations of this assumption.
%\begin{tikzpicture}[remember picture,overlay]
%\node[anchor=north east,yshift=-78pt,xshift=0pt] at (current page.north east) {\includegraphics[width=.43\textwidth]{../figs/paperind.png}};
%\end{tikzpicture}
\end{ftstf}

%=====

\begin{ftst}
{Multiple comparisons}
{The need for multiple comparisons}
If the ANOVA assumptions are verified (i.e., if we have solid grounds for trusting the result of the test), we usually need to determine \textit{which} levels of the factor are significantly different\footnote[1]{\tiny Of course this is only necessary if we rejected $H_0$ in the original test. For more on how to proceed with nonsignificant results, see Ellis(2010).};
\vone
Whenever possible, the planning of which comparisons will be after an analysis of variance procedure should be defined \textit{a priori}. Post-hoc definition of hypotheses (a.k.a.  HARKing\footnote[2]{\tiny\textbf{H}ypothesizing \textbf{A}fter the \textbf{R}esults are \textbf{K}nown. See Kerr(1998).}) are a common entry point for researcher biases into the analysis, and should be dealt with very carefully.
\end{ftst}

%=====

\begin{ftst}
{Multiple comparisons}
{Types of comparisons}
The planning of multiple comparisons must be guided by the technical question underlying the experiment.
\vone
Whenever possible, the researcher should opt to perform the smallest number of comparisons needed to adequately answer his or her question. This will require the smallest sample size, or result in the largest power for a given experimental setup.
\vone
Usual questions involve (but are not limited to):

\vhalf
\bitems\textit{How does one level compare to the others?}
%\spitem\textit{How does each level compare to the grand mean?}
\spitem\textit{How do the levels compare to each other (all vs. all)?}
\eitem
\end{ftst}

%=====

\begin{ftst}
{Multiple comparisons}
{All vs. all}
Pairwise comparisons of the \textit{all vs. all} type appear whenever we are simply interested in detecting which levels are significantly different from which, without any prior information or special interest in one specific level or ordering.
\vone
In these cases, the number of comparisons is $K=a(a-1)/2$, where\\$a$ is the number of levels.
\vone
%The sample size calculations for this case can follow the same equations used for the t test for two independent samples, but with the $\alpha$ value corrected for multiple hypotheses and the number of degrees of freedom of the reference distribution equal to those of the residual term in the ANOVA, i.e., $a(n-1)$.
To perform \textit{all vs. all} multiple comparisons, a common approach is to use Tukey's \textit{Honest Significant Difference} (HSD) approach. This method provides a slightly higher power than performing multiple t-tests with adjusted values of $\alpha$.%\footnote[4]{\tiny The difference is due to Tukey's approach using a modified value for the $t_{\beta}$ term in the power calculations. See Mathews (2011) and Montgomery (2010) for details.}. 
\end{ftst}

%=====

\begin{ftstf}
{Multiple comparisons}
{All vs. all}
Suppose that was the case for our wood fiber example:

\begin{rcode}
> library(multcomp)
> mc1    <- glht(my.model, linfct = mcp(Fiber.type = "Tukey"))
> mc1_CI <- confint(mc1, level = 0.95)

Simultaneous Confidence Intervals
Multiple Comparisons of Means: Tukey Contrasts
95% family-wise confidence level

Linear Hypotheses:
           Estimate lwr     upr    
B - A == 0 -1.9867  -4.6478  0.6745
C - A == 0  3.6500   0.9889  6.3111
D - A == 0  2.2333  -0.4278  4.8945
C - B == 0  5.6367   2.9755  8.2978
D - B == 0  4.2200   1.5589  6.8811
D - C == 0 -1.4167  -4.0778  1.2445

> plot(mc1_CI)
\end{rcode}
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south east,yshift=0pt,xshift=5pt] at (current page.south east) {\includegraphics[width=.42\textwidth]{../figs/papertukey.png}};
\end{tikzpicture}
\end{ftstf}

%=====

\begin{ftst}
{Multiple comparisons}
{All vs. one}
Pairwise comparisons of the \textit{all vs. one} type usually emerge in the context of comparing levels against a control:
\vhalf
\bitems Comparison of a proposed approach vs. existing ones;
\vhalf\item Comparison of different approaches vs. a standard one (or a placebo-like group);
\eitem
\vhalf
In these cases, the number of comparisons is $K = a-1$, where $a$ is the number of levels. %Each test can again be performed (at least in principle) using the $t_0$ test statistic:

%\beqs
%t^i_0 = \frac{\bar{y}_i - \bar{y}_0}{\hat{\sigma}\sqrt{\left(\frac{1}{n_i}+\frac{1}{n_0}\right)}}
%\eqs
\end{ftst}

%=====

%\begin{ftst}
%{Multiple comparisons}
%{All vs. one}
%There are two main approaches to calculating sample size for \textit{all vs. one} comparisons:
%
%\bitems Balanced design;
%\spitem Optimal allocation of units.
%\eitem
%\vone
%With a balanced design (that is, all levels have the same number of observations), the calculation of $n$ follows the same approach as the \textit{all vs. all} comparisons, but correcting $\alpha$ for only $a-1$ comparisons.
%\vone
%For the optimal allocation of units, an unbalanced design is used.
%\end{ftst}
%
%%=====
%
%\begin{ftst}
%{Multiple comparisons}
%{All vs. one - optimal allocation}
%As several levels will be compared against the single control group, the relative importance of the latter is greater and therefore it should have a larger sample size.
%\vhalf
%To maximize the power of this multiple comparisons procedure, the sample size of the control group should be:
%\beqs
%n_0 = n_i\sqrt{K}
%\eqs
%\vhalf
%\noindent where $n_i$ is the common sample size for the non-control levels:
%\beqs
%n_i \approxeq \left(1 + \frac{1}{\sqrt{K}}\right)\left(\frac{(t_{(\alpha_{adj}/2)}+t_{\beta})\hat{\sigma}}{\delta^*}\right)^2
%\eqs
%\lfr{A good free software for doing sample size calculations and power analysis in nontrivial contexts such as this one is G*Power 3, \url{http://www.gpower.hhu.de/} . It is also relatively simple to implement these calculations in R.}
%\end{ftst}
%
%%=====
%
%\begin{ftst}
%{Multiple comparisons}
%{All vs. one - \textbf{Dunnett's test}}
%As in the case of \textit{all vs. all} comparisons, there is a test that is usually employed for its superior sensitivity: Dunnett's test.
%\vone
%The control group sample size $n_0$ calculated assuming that Bonferroni-corrected t-tests will be used is slightly overestimated in relation to the required $n_0$ for Dunnett's test, but in practice the differences are small enough not to matter;
%\end{ftst}

%=====

\begin{ftstf}
{Multiple comparisons}
{All vs. one - \textbf{Dunnett's test}}
As in the case of \textit{all vs. all} comparisons, there is a test that is usually employed for its superior sensitivity: Dunnett's test.
\vhalf
Assuming that in our example the \textit{B} level is the standard one, against which the other ones are to be compared:
\vhalf
\begin{rcode}
> paper$Fiber.type <- relevel(paper$Fiber.type, ref = "B")
> model2           <- aov(TS.kPa ~ Fiber.type, data = paper)
> mc2              <- glht(model2, linfct = mcp(Fiber.type = "Dunnett"))
> mc2_CI           <- confint(mc2, level = 0.95)

Simultaneous Confidence Intervals
Multiple Comparisons of Means: Dunnett Contrasts
95% family-wise confidence level
Linear Hypotheses:
Estimate lwr     upr    
A - B == 0  1.9867  -0.4275  4.4008
C - B == 0  5.6367   3.2225  8.0508
D - B == 0  4.2200   1.8058  6.6342

> plot(mc2_CI)
\end{rcode}
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south east,yshift=0pt,xshift=4pt] at (current page.south east) {\includegraphics[width=.35\textwidth]{../figs/paperdunnett.png}};
\end{tikzpicture}
\end{ftstf}

%=====

\begin{ftst}
{Multiple comparisons}
{Some considerations on sample sizes}
The kind of comparisons that are to be performed after an ANOVA should be planned in advance, as it influences your data collection and sample size calculations. There are of course sample size formulas for the pure ANOVA, but these are often of limited use since they require the specification of counterintuitive parameters (such as MRES in terms of the ratio of within-groups to across-groups variances).
\vone
There are a myriad of approaches for post-ANOVA multiple comparisons\footnote[5]{\tiny Check Hothorn \textit{et al.} (2008) for an idea on how varied this can get.}, but in general the formulas for sample size calculation will follow the ideas outlined below: correct the $\alpha$ value to account for type-I error inflation and calculate $n$ based on formulas for two-sample $t$ tests. 
\end{ftst}

%=====

\begin{ftst}
{Multiple comparisons}
{Some considerations on sample sizes}
A usual, simpler approach for sample size calculations in the context of the one-way ANOVA is to perform the estimation based on the multiple comparisons one wishes to perform in case an effect is detected.
\vhalf
To perform these calculations we need to account for the fact that we will be testing multiple hypotheses on the same data, and correct the type-I error rate accordingly.
\end{ftst}

%=====

\begin{ftst}
{Multiple comparisons}
{MHT considerations}
The multiple comparisons performed after an ANOVA can be thought of as a series of t-tests (with some slight modifications) on the difference between two population means;
\vone
If the assumptions of the ANOVA are verified, we already have some information about the data: we know, for instance, that the groups are homoscedastic, and that their common variance is estimated by $MS_E$, with $a(n-1)$ degrees of freedom;\footnote{\tiny This value is the one that should be used in sample size calculations. There are ways to estimate sample sizes for the omnibus ANOVA, but we'll usually want to pinpoint the differences anyway.}.
\vone
We also know that the probability of a type-I error on \textbf{each test} is given as $\alpha$. If we want to to keep our overall error rate controlled at a given level while we perform multiple tests on the same data set, we will need to correct the $\alpha$ value used for each test.
\end{ftst} 

%=====

\begin{ftst}
{Multiple comparisons}
{MHT corrections}
There are a number of ways of adjusting the $\alpha$ value of the pairwise comparisons in order to maintain the \textit{familywise error rate} (FWER) at a controlled level\footnote[3]{\tiny The methods presented here work well when the number of comparisons is relatively small. For more on MHT, see Schaffer(1995)'s discussion on controlling the False Discovery Rate.}.
\vhalf
Two of the most common (and most conservative) are the Bonferroni and the \v{S}id\'ak corrections, which have different formulations but tend to provide very similar results.
\vhalf
Assuming $K$ planned comparisons, the Bonferroni method tests each individual hypothesis with:
\beqs
\alpha_{adj}=\frac{\alpha_{\mbox{\scriptsize \textit{family}}}}{K}
\eqs
%\vhalf
%\noindent while the \v{S}id\'ak correction uses:
%\beqs
%\alpha_{adj}=1 - \left(1-\alpha_{\mbox{\scriptsize \textit{family}}}\right)^{1/K}
%\eqs
\end{ftst}

%=====

\begin{ftst}
{Multiple comparisons}
{All vs. all}
A simple approach is to calculate the sample size using Bonferroni-corrected $\alpha$-values (for simplicity), and performing the actual pairwise tests using Tukey's or Dunnett's methods (for increased power).
\vhalf
An approximate formula to calculate balanced sample sizes is:
\beqs
n \approxeq 2\left[\left(t^{a(n-1)}_{\alpha_{adj}/2}+t^{a(n-1)}_{\beta}\right)d^*\right]^2
\eqs
\vhalf
\noindent where $d^*$ is the (standardized) MRES for the comparison of two groups.
\end{ftst}

%=====


\begin{ftst}
{More on sample sizes}
{Sample size formulas for ANOVA}
If one is interested in calculating the required sample size directly for the ANOVA procedure (without worrying about the eventual post-hoc testing), the formulas are almost as simple as those used for the t tests.
\vone
Essentially, the power/sample size calculations for the ANOVA boil down to the equality:

\beqs
F_{(1-\alpha)} = F_{\beta;\phi}
\eqs

with both $F$ distributions having $(a-1)$ degrees of freedom in the numerator and $a(n-1)$ in the denominator. The noncentrality parameter $\phi$ is given by:

\beqs
\phi = \frac{n\sum\limits_{i=1}^{a}\tau_i^2}{\hat{\sigma}^2}
\eqs
\end{ftst}

%=====

\begin{ftst}
{More on sample sizes}
{Sample size formulas for ANOVA}
To ilustrate the sample size calculation procedure, imagine an experimental design with $a = 4,\ \alpha = 0.05$, an upper estimate of the within-groups standard deviation $ \sigma = 7$, and suppose that the researcher wants to be able to detect whether any two means present differences of magnitude $\delta^* = 12$ with power $(1-\beta)=0.8$.
\vone
Under these conditions, we can assume a hypothetical (conservative) scenario where we have two levels biased symmetrically about the grand mean, and all the others equal to zero:

$$ \tau = \left\{-\frac{\delta^*}{2}, \frac{\delta^*}{2}, 0, 0\right\}$$

\end{ftst}

%=====

\begin{ftst}
{More on sample sizes}
{Sample size formulas for ANOVA}
For this case we have a noncentrality parameter:

$$\phi = \frac{4\left(6^2+6^2+0+0\right)}{7^2} = 5.88$$

Which allows us to calculate the required sample size by iterating on $n$ until:

$$F_{(1-\alpha)} \leq F_{\beta;\phi}$$

\end{ftst}

%=====

\begin{ftstf}
{More on sample sizes}
{Sample size formulas for ANOVA}
Doing it manually:
\begin{rcode}
> a       <- 4
> alpha   <- 0.05
> sigma   <- 7
> delta   <- 12
> beta    <- 0.2
>
> tau <- c(-delta/2, delta/2, rep(0, a - 2)) # define tau vector
> n   <- 2
> while (qf(1 - alpha, a - 1, a*(n - 1)) > 
+        qf(beta, a - 1, a*(n - 1), n*sum(tau^2)/sigma^2)) n <- n + 1
> print(n)
[1] 9
\end{rcode}
\vhalf
Using \verb|power.anova.test()|:
\begin{rcode}
> vartau <- var(tau)
> power.anova.test(groups = 4, between.var = vartau, 
+                  within.var = sigma^2, sig.level = alpha, 
+                  power = 1 - beta)$n
[1] 8.463358
\end{rcode}
\end{ftstf}

%\begin{ftstf}
%{More on sample sizes}
%{Sample size formulas for ANOVA}
%The second case (one level biased in relation to all others) is also quite easy to calculate manually, but lets keep it simple:
%
%\begin{rcode}
%> tau <- c(-delta*(a - 1)/a, rep(delta/a, a - 1))
%> vartau <- var(tau)
%> power.anova.test(groups = 4, between.var = vartau, 
%+                  within.var = sigma^2, sig.level = alpha, 
%+                  power = 1 - beta)$n
%[1] 6.018937
%\end{rcode}
%\vone
%It is important to remember that these are the sample sizes required for the ANOVA only - any multiple comparisons procedure executed afterwards to pinpoint the significant differences will have smaller power for same-sized effects (unless more observations are added). This is one reason why it is common to design experiments calculating the sample sizes based on the multiple comparisons procedure, instead of using the ANOVA formulas.
%\end{ftstf}

\begin{ftst}
{Bibliography}
{\ }
\scriptsize
\textbf{Required reading}

\benums  D.C. Montgomery, G.C. Runger, \textit{Applied Statistics and Probability for Engineers}, Ch. 13. 5th ed., Wiley, 2010.
\item N.L. Kerr, \textit{HARKing: Hypothesizing After the Results are Known}, Personality and Social Psychology Reviews 2(3): 196--217, 1998.
\eenum

\textbf{Recommended reading}

\benums P. D. Ellis, \textit{The Essential Guide to Effect Sizes}. 1st ed., Cambridge, 2010.
\item J.P. Schaffer, \textit{Multiple Hypothesis Testing}, Annual Reviews on Psychology 46, 561--584, 1995.
\item P. Mathews, \textit{Sample Size Calculations: Practical Methods for Engineers and Scientists}. Ch. 8, 1st ed., MMB, 2010.
\item T. Hothorn, F. Bretz, P. Westfall, \textit{Simultaneous Inference in General Parametric Models}. Biometrical  Journal 50(3), 346--363, 2008.
\eenum
\end{ftst}

%=====

\input{../defs/FinalSlide.tex}

\end{document}
